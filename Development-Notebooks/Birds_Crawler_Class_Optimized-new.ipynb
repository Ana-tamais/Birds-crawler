{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import urllib\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "import requests\n",
    "import time\n",
    "import threading\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdCrawler:\n",
    "    \"\"\"\n",
    "    Class of crawler of wikiaves.com.br. \n",
    "    It works with the following approach: It gets a initial link and gather all species id (10001 ~ 12000). \n",
    "    Then it generates a link which contains a json with all information we need (.jpg links on s3 amazon,\n",
    "    number of photos from each specie and the specie's name. The .jpg links for each specie will gonna be\n",
    "    exported by a .txt on the format ['link', id] with a lot of links. This information is going to be catched\n",
    "    by the go program and saved on the local memory.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, store_path = '',\n",
    "                 initial_link = 'https://www.wikiaves.com.br/especies.php?t=t',\n",
    "                 firefox_path = 'geckodriver'):\n",
    "        \"\"\"\n",
    "        attributes:\n",
    "        \n",
    "        specie: dictionary which has the following format {id: specie name}\n",
    "        count_photo: dictionary which has the following format {id: count of the number of photos from this specie}\n",
    "        path: path where all the data is going to be stored\n",
    "        browser: we are going to use it to navigate on internet\n",
    "        soup: use this to work with html code and gather page's information (the number of birds species)\n",
    "        num_species: number of birds species (1890 on 10/02/2020)\n",
    "        firefox_path: where geckodriver is stored\n",
    "        my_filename: name of the .txt which contains all .jpg links from one specie\n",
    "        file_photo: dictionary which has the following format {id: open(some_path, 'w')}\n",
    "        r: used to get the json from the page and work with it to get the information we need (.jpg links,\n",
    "                                                                                               count of photos\n",
    "                                                                                               and specie's name)\n",
    "        pag: count the page's number from the link where the json is stored\n",
    "        count: iterate the number of the .jpg link we are saving\n",
    "        \"\"\"\n",
    "        \n",
    "        self.specie = {}\n",
    "        self.count_photo = {}\n",
    "        self.path = store_path\n",
    "        self.browser = None\n",
    "        self.soup = None\n",
    "        self.num_species = None\n",
    "        self.firefox_path = firefox_path\n",
    "        self.initial_link = initial_link\n",
    "        self.my_filename = {}\n",
    "        self.file_photo = {}\n",
    "        self.r = {}\n",
    "        self.pag = {}\n",
    "        self.count = {}\n",
    "        \n",
    "    def connect_to_internet(self):\n",
    "        \"\"\"\n",
    "        It uses selenium to connect to internet\n",
    "        \"\"\"\n",
    "        firefox_profile = webdriver.FirefoxProfile()\n",
    "        options = Options()\n",
    "        options.add_argument('--headless')\n",
    "        self.browser = webdriver.Firefox(firefox_profile=firefox_profile,\n",
    "                                         options=options,\n",
    "                                         executable_path=self.firefox_path)\n",
    "    def get_num_species(self):\n",
    "        \"\"\"\n",
    "        Get the number of birds species (1890 on 10/02/2020)\n",
    "        \"\"\"\n",
    "        self.browser.get(self.initial_link)\n",
    "        html = self.browser.page_source\n",
    "        self.soup = BeautifulSoup(html, 'html.parser')\n",
    "        self.num_species = self.soup.find_all(class_ = 'font-blue-soft')[-1].text[:4]\n",
    "        self.num_species = int(self.num_species)\n",
    "            \n",
    "    def create_dir_linksimage(self):\n",
    "        \"\"\"\n",
    "        It creates the path where we are going to store the .jpg links\n",
    "        \"\"\"\n",
    "        try:\n",
    "            os.mkdir(self.path + '/links_image')\n",
    "        except:\n",
    "            print('/links_image already exist')\n",
    "\n",
    "    \n",
    "    def get_id(self):\n",
    "        \"\"\"\n",
    "        It creates the id for 1890 species based on the site's id (10001 until 11890)\n",
    "        \"\"\"\n",
    "        for k in range(1, self.num_species+1):\n",
    "            self.count_photo['{}'.format(10000 + k)] = None\n",
    "            self.specie['{}'.format(10000 + k)] = None\n",
    "            \n",
    "    def create_dependencies(self, id_):\n",
    "        \"\"\"\n",
    "        Create some dependent variables to not have the concurrency problem in the threads\n",
    "        \"\"\"\n",
    "        self.my_filename['{}'.format(id_)] = os.path.join(self.path + '/links_image/links_{}.txt'.format(id_))\n",
    "        self.file_photo['{}'.format(id_)] = open(self.path + '/links_image/links_{}.txt'.format(id_), 'w')\n",
    "        self.r['{}'.format(id_)] = requests.get('https://www.wikiaves.com.br/getRegistrosJSON.php?tm=f&t=s&s={}&o=mp&o=mp&p=1'.format(id_))\n",
    "        try:\n",
    "            self.r['{}'.format(id_)] = self.r['{}'.format(id_)].json()\n",
    "        except ValueError:\n",
    "            print(\"No json, id = \", id_)\n",
    "    \n",
    "    def count_photo_and_specie(self, id_):\n",
    "        \"\"\"\n",
    "        It gets the number of photos and the specie's name\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.count_photo['{}'.format(id_)] = self.r['{}'.format(id_)]['registros']['total']\n",
    "            self.specie['{}'.format(id_)] = self.r['{}'.format(id_)]['registros']['itens']['1']['sp']['idwiki']\n",
    "        except:\n",
    "            print(\"no image, id = \", id_)\n",
    "            \n",
    "    def get_json(self, id_):\n",
    "        \"\"\"\n",
    "        It does a request to the page we want to get the json and \n",
    "        set count to 1 because we are starting a new page\n",
    "        \"\"\"\n",
    "        self.r['{}'.format(id_)] = requests.get('https://www.wikiaves.com.br/getRegistrosJSON.php?tm=f&t=s&s={}&o=mp&o=mp&p={}'.format(id_, str(self.pag['{}'.format(id_)])))\n",
    "        self.r['{}'.format(id_)] = self.r['{}'.format(id_)].json()\n",
    "        self.count['{}'.format(id_)] = 1\n",
    "    \n",
    "    def get_image_links(self, id_):\n",
    "        \"\"\"\n",
    "        It uses the previous functions (create_dependencies, count_photo_and_specie) to start getting the .jpg \n",
    "        links. Then iterate for all json pages to get all .jpg links from an specie, and save it on a .txt file\n",
    "        with the following format: ['.jpg link', id]\n",
    "        \"\"\"\n",
    "        self.create_dependencies(id_)\n",
    "        self.count_photo_and_specie(id_)\n",
    "        self.pag['{}'.format(id_)] = 1\n",
    "        try:\n",
    "            while self.r['{}'.format(id_)]['registros']['itens'] != {}:\n",
    "                self.get_json(id_)\n",
    "                while self.count['{}'.format(id_)] < 22:\n",
    "                    try:\n",
    "                        self.file_photo['{}'.format(id_)].write(\"['\" + self.r['{}'.format(id_)]['registros']['itens']['{}'.format(str(self.count['{}'.format(id_)]))]['link'].replace('#', 'q') + \"'\" + ', {}]'.format(id_) + '\\n')\n",
    "                    except KeyboardInterrupt:\n",
    "                        print('KeyboardInterrupt')\n",
    "                        break\n",
    "                    except requests.exceptions.ConnectionError:\n",
    "                        print('Timeout, id = ', id_)\n",
    "                    except:\n",
    "                        break\n",
    "                    self.count['{}'.format(id_)] += 1\n",
    "                self.pag['{}'.format(id_)] += 1\n",
    "            self.file_photo['{}'.format(id_)].close()\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print('Timeout, id = ', id_)\n",
    "        except:\n",
    "            print(\"No json, id = \", id_)\n",
    "            \n",
    "    def get_all_links(self, ids):\n",
    "        \"\"\"\n",
    "        It gets all .jpg links from a list of species and does it sequentially\n",
    "        \"\"\"\n",
    "        for id_ in ids:\n",
    "            self.get_image_links(id_)\n",
    "        \n",
    "    def crawl(self, species):\n",
    "        \"\"\"\n",
    "        Crawl function is our main function. It is where all the previous functions are used to extract all\n",
    "        the .jpg links from api\n",
    "        \"\"\"\n",
    "        inicio = time.time()\n",
    "        self.connect_to_internet()\n",
    "        self.get_num_species()\n",
    "        self.create_dir_linksimage()\n",
    "        self.get_id()\n",
    "        self.get_all_links(species)\n",
    "        self.browser.close()\n",
    "        fim = time.time()\n",
    "        print(fim-inicio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/links_image already exist\n"
     ]
    }
   ],
   "source": [
    "classe = BirdCrawler(store_path = '/home/aninha/Documents/Birds_Project')\n",
    "classe.create_dir_linksimage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_thread(ids, store_path, count, num_threads):\n",
    "    count = int(count)\n",
    "    num_threads = int(num_threads)\n",
    "    inicio = time.time()\n",
    "    classes = {}\n",
    "    ids_species = []\n",
    "    for id_ in range(int(ids[0]), int(ids[0]) + count):\n",
    "        ids_species.append(str(id_))\n",
    "\n",
    "    threads = []\n",
    "    for id_ in range(0, len(ids_species), num_threads):\n",
    "        classes['{}'.format(id_)] = BirdCrawler(store_path = store_path)\n",
    "        threads.append(threading.Thread(target=classes['{}'.format(id_)].get_all_links, \n",
    "                                        args = ([str(int(ids_species[id_]) + k) for k in range(num_threads)]\n",
    "                                                ,)))\n",
    "    for i in threads:\n",
    "        try:\n",
    "            i.start()\n",
    "        except:\n",
    "            print(1)\n",
    "            i.stop()\n",
    "    for i in threads:\n",
    "        i.join()\n",
    "    final = time.time()\n",
    "    print(final - inicio)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
